# ğŸ” Omni-Engine - Advanced Web Scraper

A powerful, multi-engine web scraping tool that aggregates search results from multiple search engines with advanced error handling, caching, and a sleek web interface.

## âœ¨ Features

### ğŸ† **Core Capabilities**
- **Multi-Engine Search**: Simultaneously scrapes DuckDuckGo, Bing, and Yahoo
- **Advanced Error Handling**: Robust retry mechanisms and graceful failure handling
- **Intelligent Caching**: SQLite-based result caching to improve performance
- **Relevance Scoring**: Smart algorithm to rank results by relevance
- **Async Processing**: High-performance asynchronous scraping
- **Robots.txt Compliance**: Respects website scraping policies

### ğŸ¨ **Web Interface**
- **Modern Dark UI**: Sleek, responsive design optimized for all devices
- **Real-time Search**: Live search with progress indicators
- **Export Functionality**: Export results to JSON or CSV formats
- **Search History**: Track and reuse recent searches
- **Statistics Dashboard**: Monitor scraping performance and cache status
- **Mobile Responsive**: Works perfectly on phones, tablets, and desktops

### ğŸ”’ **Security & Performance**
- **Rate Limiting**: Intelligent delays to avoid being blocked
- **User Agent Rotation**: Random user agents to appear more human-like
- **Content Filtering**: Clean and structure scraped content
- **Error Recovery**: Automatic retry with exponential backoff
- **Memory Efficient**: Optimized for low resource usage

## ğŸ“š Quick Start

### Prerequisites
- Python 3.8 or higher
- pip (Python package manager)
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/namaa03id/Black-cat.git
   cd Black-cat
   ```

2. **Create a virtual environment** (recommended)
   ```bash
   python -m venv omni_env
   source omni_env/bin/activate  # On Windows: omni_env\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   python app.py
   ```

5. **Open your browser** and navigate to:
   ```
   http://127.0.0.1:5000
   ```

## ğŸ’» Usage

### Web Interface

1. **Search**: Enter your query in the search box and click "Search"
2. **Filter Results**: Adjust max results and toggle cache usage
3. **Export Data**: Use the export buttons to download results as JSON or CSV
4. **View History**: Check recent searches in the sidebar
5. **Monitor Stats**: View real-time statistics in the header

### Command Line Interface

```python
# Example: Direct usage of the scraper
from omni_scraper import OmniScraper
import asyncio

async def main():
    scraper = OmniScraper()
    results = await scraper.search("Python web scraping", max_results=20)
    
    for result in results:
        print(f"Title: {result.title}")
        print(f"URL: {result.url}")
        print(f"Source: {result.source}")
        print(f"Score: {result.relevance_score}")
        print("---")

asyncio.run(main())
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|--------------|
| `/search` | POST | Perform a web search |
| `/export` | POST | Export search results |
| `/stats` | GET | Get application statistics |
| `/recent` | GET | Get recent searches |
| `/clear-cache` | POST | Clear the search cache |
| `/health` | GET | Health check endpoint |

## ğŸ”§ Configuration

### Environment Variables

```bash
# Optional configuration
export OMNI_HOST=127.0.0.1
export OMNI_PORT=5000
export OMNI_DEBUG=False
export OMNI_MAX_RETRIES=3
export OMNI_DELAY_MIN=1
export OMNI_DELAY_MAX=3
```

### Advanced Options

You can customize the scraper behavior by modifying the `OmniScraper` initialization:

```python
scraper = OmniScraper(
    delay_range=(1, 3),  # Random delay between requests
    max_retries=3        # Maximum retry attempts
)
```

## ğŸ§ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Interface    â”‚
â”‚  (Flask + HTML)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Endpoints     â”‚
â”‚    (app.py)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Scraping Engine   â”‚
â”‚ (omni_scraper.py) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Search Engines    â”‚
â”‚ DuckDuckGo | Bing â”‚
â”‚    Yahoo           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure

```
Black-cat/
â”‚
â”œâ”€â”€ omni_scraper.py     # Core scraping engine
â”œâ”€â”€ app.py              # Flask web application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Main web interface
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css       # CSS styling
â”‚   â””â”€â”€ script.js       # JavaScript functionality
â”‚
â”œâ”€â”€ exports/            # Generated export files
â”œâ”€â”€ omni_cache.db       # SQLite cache database
â””â”€â”€ omni_scraper.log    # Application logs
```

## ğŸ“Š Performance

### Benchmarks

| Metric | Performance |
|--------|-------------|
| **Search Speed** | 3-8 seconds per query |
| **Concurrent Requests** | Up to 10 simultaneous |
| **Cache Hit Rate** | ~85% for repeated queries |
| **Memory Usage** | ~50-100MB average |
| **Success Rate** | >95% for most queries |

### Optimization Tips

1. **Enable Caching**: Keep cache enabled for better performance
2. **Reasonable Delays**: Don't set delays too low to avoid blocks
3. **Limit Results**: Use appropriate max_results for your needs
4. **Monitor Logs**: Check logs for blocked domains or errors

## ğŸš« Ethical Usage

### Best Practices

- **Respect robots.txt**: The scraper automatically checks and respects robots.txt
- **Use reasonable delays**: Don't overload target servers
- **Limit request frequency**: Avoid rapid-fire requests
- **Check ToS**: Ensure compliance with search engine terms of service
- **Use responsibly**: Don't use for spam or malicious purposes

### Legal Considerations

- This tool is for educational and research purposes
- Users are responsible for complying with applicable laws
- Respect website terms of service and copyright
- Consider using official APIs when available

## ğŸ”› Troubleshooting

### Common Issues

**1. Import Errors**
```bash
# Solution: Install missing packages
pip install -r requirements.txt
```

**2. No Results Found**
```bash
# Check if search engines are accessible
# Try different search terms
# Check internet connection
```

**3. Server Won't Start**
```bash
# Check if port 5000 is available
python app.py --port 8000  # Use different port
```

**4. Blocked by Search Engines**
```bash
# Increase delay_range in omni_scraper.py
# Clear cache and try again later
# Check if your IP is temporarily blocked
```

### Debug Mode

Run in debug mode for detailed error information:

```bash
python app.py --debug
```

## ğŸ“ Logging

The application creates detailed logs in `omni_scraper.log`:

```bash
# View logs
tail -f omni_scraper.log

# Search for errors
grep ERROR omni_scraper.log
```

## ğŸ†™ API Documentation

### Search Endpoint

```javascript
POST /search
Content-Type: application/json

{
    "query": "your search query",
    "max_results": 20,
    "use_cache": true
}
```

**Response:**
```javascript
{
    "success": true,
    "query": "your search query",
    "results_count": 15,
    "results": [
        {
            "title": "Page Title",
            "url": "https://example.com",
            "snippet": "Description...",
            "content": "Full content...",
            "source": "DuckDuckGo",
            "relevance_score": 0.85,
            "timestamp": "2024-01-01T12:00:00"
        }
    ],
    "search_time": "2024-01-01T12:00:05"
}
```

## ğŸš€ Advanced Features

### Custom Search Engines

Add new search engines by extending the `search_engines` dictionary:

```python
self.search_engines['custom_engine'] = {
    'url': 'https://custom-search.com/search',
    'params': {'q': '{query}'},
    'parser': self._parse_custom_engine
}
```

### Proxy Support

Add proxy support by modifying the aiohttp session:

```python
connector = aiohttp.TCPConnector()
async with aiohttp.ClientSession(
    connector=connector,
    headers=self.get_headers()
) as session:
    # Your scraping code here
```

## ğŸ•°ï¸ Changelog

### Version 1.0.0 (Latest)
- âœ¨ Initial release
- âœ¨ Multi-engine search support
- âœ¨ Web interface with dark theme
- âœ¨ SQLite caching system
- âœ¨ Export functionality
- âœ¨ Comprehensive error handling
- âœ¨ Mobile responsive design

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/new-feature`
3. **Make your changes** and test thoroughly
4. **Commit your changes**: `git commit -m 'Add new feature'`
5. **Push to the branch**: `git push origin feature/new-feature`
6. **Submit a Pull Request**

### Development Setup

```bash
# Clone your fork
git clone https://github.com/your-username/Black-cat.git
cd Black-cat

# Create virtual environment
python -m venv dev_env
source dev_env/bin/activate

# Install development dependencies
pip install -r requirements.txt
pip install pytest pytest-asyncio black flake8

# Run tests
pytest

# Format code
black .
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Namaa** - [@namaa03id](https://github.com/namaa03id)

## ğŸ™ Acknowledgments

- Built with â¤ï¸ using Python, Flask, and modern web technologies
- Inspired by the need for comprehensive web scraping tools
- Special thanks to the open-source community for amazing libraries

---

<div align="center">

**ğŸ† Made with Python and a passion for data ğŸ†**

*If you find this project useful, please â­ star it on GitHub!*

</div>